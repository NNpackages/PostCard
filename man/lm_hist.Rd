% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lm_hist.R
\name{lm_hist}
\alias{lm_hist}
\title{ANCOVA models using historical data}
\usage{
lm_hist(
  data.list,
  method = c("None", "DT", "PSM"),
  sup.margin = 0,
  alpha = 0.05,
  outcome.var = "y",
  treatment.var = "w",
  adj.covs = NULL,
  pred.model = NULL,
  interaction = FALSE,
  est.power = FALSE,
  ATE = NULL,
  L2 = FALSE,
  B = 100,
  N.cores = 4
)
}
\arguments{
\item{data.list}{A list of elements $hist and $rct, which are both lists of elements being historical and current RCT data sets, respectively. A $hist_test element of the list can be provided for prospective power calculations. Fx. this object could be the output of generate.data.}

\item{method}{Method for using historical data. Options: None, DT, PSM. None refers to ANOVA (for adj.covs = NULL) and ANCOVA (for adj.covs specified).}

\item{sup.margin}{Superiority margin (for non-inferiority margin, a negative value can be provided).}

\item{alpha}{Significance level. Due to regulatory guidelines when using a one-sided test, half the specified significance level is used. Thus, for standard alpha = .05, a significance level of 2.5\\% is used.}

\item{outcome.var}{Character with the name of the outcome variable in both the $rct and $hist data set.}

\item{treatment.var}{Character with the name of the outcome treatment indicator in both the $rct and $hist data set. Notice that the treatment variable should be an indicator with treatment == 1 and control == 0.}

\item{adj.covs}{Character vector with names of the covariates to adjust for as raw covariates in the ANCOVA model for estimating the ATE. Make sure that categorical variables are considered as factors.}

\item{pred.model}{Model object which should be a function that fits a prediction model based on the baseline covariates. This is only needed for method == "DT" or method == "PSM"}

\item{interaction}{Logical value, that determines whether to model interaction effects between covariates and treatment indicator when estimating the ATE. For method = "DT", the prognostic score is regarded as an additional covariate and thus the interaction between the prognostic score and the treatment indicator is included.}

\item{est.power}{Logical value. If set to TRUE, prospective power calculation is carried out based on theoretical non-centrality parameter as well as Guenther-Schouten approximations, using historical data "hist" and "hist_test" provided in data.list. The entities sigma_2, rho, and R2 are calculated using the historical data, and these can not be specified by the user. Look at NC_power or GS_power if you want a power calculation based on user specified entities. For method="DT", only "hist_test" data is used since "hist" data is used for training the model. In addition, the necessary entities for these calculations are outputted.}

\item{ATE}{The average treatment effect. The value is only needed if est.power == TRUE or simulation.study == TRUE. If est.power == TRUE this value is the minimum effect size that we should be able to detect. If simulation.study == TRUE this should be the actual average treatment effect, that the data was simulated from. If data was simulated using the generate.data function the value is set equal to the ATE attribute from the data set.}

\item{L2}{Logical value. Only relevant if method = "DT" is specified. If set to TRUE, average squared difference between true and estimated prognostic scores are estimated as column "L2". The difference is estimated on elements in data.list$rct. Irrelevant for Oracle estimators (since these use the true prognostic score).}

\item{B}{Only relevant for method = PSM. Number of bootstraps for estimating bias between HC and CC groups.}

\item{N.cores}{Number of cores to use for parallelisation}

\item{simulation.study}{Logical value. If set to TRUE the coverage, MSE and type1.err is calculated for each pair of RCT and historical data sets.}
}
\value{
The function returns a data set where each row is the estimated entities on each combination of historical and current RCT data. That means
that there will be N.sim rows if data was simulated using the generate_data function. Thus, column means can be taken to get averaged results across the different datasets.
}
\description{
The function estimates the ATE, std. error of ATE, coverage, power, type1 error
rate, and approximations of power for a pair of data sets in data.list. The output
of the function is a data set with each row being the estimated entities on
each combination of historical and current RCT data from the data.list object.
Thus, column means can be taken to get averaged results across the different
data sets.
}
\details{
The prospective power estimations are determined by the NC_power and GS_power functions. Look at the details of these to see
specifically how the power is determined. If interaction = TRUE and there is historical treatment group participants the interaction
terms are included in  the calculation of R2, otherwise this is not included and hence the power is conservatively estimated.

The coverage is calculated as the proportion of times the true ATE was inside the confidence intervals

\deqn{\left[\widehat{\mathrm{ATE}}_i- t_{0.975, n-k}\sqrt{\mathbb{V}\mathrm{ar}\left(\widehat{\mathrm{ATE}}_i\right)},\;\; \widehat{\mathrm{ATE}}_i+t_{0.975, n-k}\sqrt{\mathbb{V}\mathrm{ar}\left(\widehat{\mathrm{ATE}}_i\right)}\right]}

for i=1,2,3,...,N.sim, where \eqn{\widehat{\mathrm{ATE}}_i} and \eqn{\mathbb{V}\mathrm{ar}\left(\widehat{\mathrm{ATE}}_i\right)} are the ATE and
variance estimates from the ith data set, and \eqn{t_{0.975, n-k}} is the 97.5\\%-quantile of the t-distribution with degrees of
freedom equal to the sample size n minus the number of columns in the design matrix. Using the 97.5\\%-quantile, we specify a
significance level of 2.5\\% for the one-sided superiority test that we want to perform, which corresponds to specification
of a 5\\% significance level for a two-sided test, and we would therefore expect an estimated coverage of 95\\%.

In order to empirically estimate the probability of correctly rejecting this null hypothesis (the power) we evaluate
if (estimate - sup.margin)/std.err > crit.val.t for each pair of RCT and historical data. This TRUE/FALSE variable is
given in the t.test output. For multiple pairs of RCT and historical data the power is estimated as the proportion of
times t.test == TRUE.

In order to empirically estimate the probability of mistakenly rejecting the null hypothesis (the type I
error probability), we alter the simulated data by subtracting \eqn{\mathrm{ATE}-\Delta_s} from the outcome variable for
patients in the treatment group, such that the new \eqn{\mathrm{ATE}} was equal to the superiority margin (the case of
correct null hypothesis which has largest probability of rejection). Subtracting \eqn{\mathrm{ATE}-\Delta_s} from the
outcome variable for patients in the treatment group corresponds to shifting the estimated treatment effect by
\eqn{\mathrm{ATE}-\Delta_s}. For multiple pairs of RCT and historical data the type I error probability is then estimated
by calculating the number of times we (incorrectly) rejected the null hypothesis from the $t$-test statistic.
}
\examples{

data <- generate_data(N.sim = 10, N.hist.control = 100, N.hist.treatment = 100,
              N.control = 50, N.treatment = 50)

lm_hist(data)

}
